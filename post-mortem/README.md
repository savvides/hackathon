# Post-Mortem

Unfiltered analysis of what happened, what worked, and what didn't. Published so other organizers can learn from our first run.

## Files

**`results.md`** — Final rankings for 9 teams, scoring methodology, and product descriptions. Student names have been anonymized. Shows the scoring distribution and the 21-voter peer evaluation system.

**`student-feedback-analysis.md`** — The most operationally valuable document in this folder. 16 of 28 participants responded (57%). Key findings:

- NPS of +38 (healthy for first-run with pre-assigned teams and mandatory ethics framework)
- Team collaboration quality is the dominant variable explaining satisfaction variance: 3.1-point NPS gap between effective and struggling teams
- Pre-assignment model works; the matching algorithm needs more granular inputs
- PI workshop quality directly predicts framework adoption (perfect correlation)
- All structural elements scored above 4.25/5

**`mentor-feedback-report.md`** — 6 of 25 mentors responded (24% — the response rate itself is a finding). Core insights:

- 100% would return, 100% said projects met expectations
- Problem scoping was the #1 student need (unanimous)
- Station-expertise matching scored highest (4.80/5)
- Utilization imbalance: 50% of mentors saw 0–3 teams (routing problem, not quality problem)
- Day 1 roaming coverage critically understaffed (1 mentor for 25 teams)

## Why This Matters for Other Organizers

The biggest risk in a pre-assigned-team hackathon is team dysfunction cascading into every other metric. Our data shows this clearly: the two detractors both had struggling teams, and both requested better skills-based matching. The pre-assignment concept isn't the problem — insufficient matching signal is.

The second biggest operational finding: mentor value is high, but discovery is the bottleneck. Visible station signage, mandatory mentor visit requirements, and table-based (not booth-based) seating are low-cost fixes with high impact.
