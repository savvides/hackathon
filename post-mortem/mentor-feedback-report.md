**MENTOR FEEDBACK REPORT**

ScaleU + Principled Innovation Academy Hackathon 2026

February 2-4, 2026 \| 1951@Skysong, Scottsdale, AZ

Version: **1.0** \| Prepared: February 5, 2026

  -----------------------------------------------------------------------

  -----------------------------------------------------------------------

1\. Executive Summary

This report analyzes mentor feedback from the ScaleU + Principled
Innovation Academy Hackathon 2026. Of 28 mentors recruited and 25 who
participated on-site, 6 submitted post-event feedback (24% response rate
of attendees). While the low response rate limits statistical
generalizability, the data reveals clear patterns: high satisfaction
with the mentoring experience, universal intent to return, and
consistent identification of problem scoping as students' primary need.
The most significant structural finding is a utilization
imbalance---half of responding mentors saw 0-3 teams, suggesting either
low student demand for mentoring or friction in the station-based
format.

| **6 / 25** Response Rate (24%) |  | **100%** Would Return |  | **100%** Projects Met Expectations |  | **100%** Top Need: Problem Scoping |
| --- | --- | --- | --- | --- | --- | --- |


2\. Mentor Participation Analysis

2.1 Recruitment-to-Participation Funnel

  ----------------------- ----------- ----------- ----------- ------------
  **Stage**               **Count**   **% of      **% of      **Status**
                                      Prior**     Total**     

  Signed Up               **28**      ---         100%        Baseline

  Showed Up               **25**      89.3%       89.3%       Good

  Submitted Feedback      **6**       24.0%       21.4%       Low
  ----------------------- ----------- ----------- ----------- ------------

The 89.3% show rate is strong for a volunteer commitment, with only 3
no-shows from the recruited pool. However, the 24% feedback response
rate is a critical gap. 19 mentors who attended provided no post-event
data, making this report's conclusions directional rather than
definitive. The roster shows 27 unique mentors were assigned (Casey
Evans had no day checkmarks), so the 28 signup vs. 27 assignment
discrepancy suggests either a late withdrawal or a data entry gap.

2.2 Station Coverage vs. Team Demand

  --------------------- ------------ ------------ ------------ ------------
  **Station**           **Day 1      **Day 2      **Total      **Teams per
                        Mentors**    Mentors**    Mentors**    Mentor\***

  **Station 1:          7            6            9            \~2.8
  Education & Ops**                                            

  **Station 2: Student  4            4            7            \~3.6
  Wellness**                                                   

  **Station 3: Health & 6            5            8            \~3.1
  AI/Tech**                                                    

  **Roaming**           1            3            3            N/A

  **TOTAL**             **18**       **18**       **27**       
  --------------------- ------------ ------------ ------------ ------------

*\*Teams per Mentor assumes 25 teams distributed evenly across 3
stations. Actual distribution depends on problem selection.*

Station 1 had the heaviest staffing (9 total mentors), while Roaming was
critically understaffed on Day 1 with only 1 mentor ([Mentor 26])
covering floor circulation for all 25 teams. This is a structural
weakness: the design document states roaming mentors should \"check on
teams that haven't sought help\"---a single roamer cannot cover 25
tables effectively during a 4-hour window.

3\. Quantitative Feedback Analysis

3.1 Likert Scale Ratings (1-5)

  --------------------------- ---------- --------- --------- --------- --------------
  **Dimension**               **Mean**   **Min**   **Max**   **n**     **Signal**

  Station matched my          **4.80**   4         5         5         **Strong**
  expertise                                                            

  Session length was          **4.67**   4         5         6         **Strong**
  appropriate                                                          

  Had enough context about    **4.20**   3         5         5         **Moderate**
  problems                                                             

  Station-based format worked **3.80**   1         5         5         **Mixed**
  well                                                                 
  --------------------------- ---------- --------- --------- --------- --------------

Key Observations

Station-expertise matching scored highest (4.80), validating the
assignment process. The assignment spreadsheet shows deliberate
placement based on mentor-declared interest areas, and this paid off.
Session length (15-20 min) also rated well at 4.67---no one flagged it
as too short or long.

The two lower-scoring dimensions reveal actionable gaps. \"Context about
problems\" (4.20) had one mentor rate it at 3, and the open-ended
feedback explicitly requests a handout of team projects before mentoring
begins. \"Station-based format\" (3.80) contains the survey's only score
of 1---a strong outlier. Combined with the open-ended comment about
booth versus table seating, this signals that the physical setup created
friction for at least some mentors.

The variance pattern is notable: high-consensus items (expertise match,
session length) versus split-opinion items (format, context). This
suggests the structural design was sound in concept but execution
details---physical layout, pre-event briefing materials---need
tightening.

3.2 Team Engagement Volume

  ----------------------- ----------------- --------------- -----------------
  **Teams Seen**          **Respondents**   **% of          **Implication**
                                            Respondents**   

  **0-3 teams**           **3**             50%             Underutilized

  **4-6 teams**           **2**             33%             Moderate load

  **7-10 teams**          **1**             17%             Heavy engagement
  ----------------------- ----------------- --------------- -----------------

Half of responding mentors interacted with 3 or fewer teams across their
4-hour shift. With 25 teams active and 18 mentors present each day, the
theoretical capacity was high (approximately 3.6-5.4 sessions per mentor
if evenly distributed). The low-engagement cluster suggests one or more
of: teams were not proactively seeking mentors, signage/wayfinding made
stations hard to find (corroborated by open-ended feedback), or certain
stations attracted fewer teams due to problem distribution. Given that
100% of respondents identified \"problem scoping\" as the top need, the
demand existed---the bottleneck appears to be discovery and routing, not
relevance.

4\. Categorical Findings

**Help type needed:** 6/6 respondents (100%) identified \"Problem
scoping / narrowing focus\" as the most common need. Zero respondents
selected technical implementation, PI framework guidance, pitch
preparation, or team dynamics---despite these being available options.
This unanimity is the report's strongest signal: students' primary
struggle was not building the solution but defining what to build.

**Project quality:** 6/6 (100%) rated quality as \"Met
expectations---solid prototypes.\" No respondent selected above or below
expectations. This is a positive validation of the hackathon's design:
the curated problem bank, PI workshop, and team formation process
produced competent outputs.

**Return intent:** 6/6 (100%) would mentor again. This is the
highest-value retention metric---the experience was positive enough that
every respondent is willing to volunteer again despite the issues they
flagged.

5\. Qualitative Analysis: Open-Ended Feedback

4 of 6 respondents provided written feedback. Thematic clustering
reveals three distinct improvement vectors:

Theme 1: Pre-Event Briefing (2 mentions)

-   *\"A handout given to mentors ahead of time about the projects, as
    well as scope of support they can provide help with.\"*

-   *\"Having a 15-minute orientation huddle meeting would help.\"*

This aligns with the 4.20 context score. Mentors arrived without knowing
which problems teams had selected or what kind of support was expected.
A pre-shift briefing covering team-problem assignments and a one-pager
defining mentor scope would address both concerns.

Theme 2: Physical Layout and Wayfinding (1 mention, detailed)

-   *\"Have mentors sit at tables, rather than in booths. Also, mark the
    tables with the names of tracks (simple table tent could do). For
    those not familiar with the location...provide more information
    about how to get to the actual space---especially if there are
    several entrances (more signage would have been helpful).\"*

This is the most operationally specific feedback and likely explains the
format score of 1 from one respondent. The booth setup created a
physical barrier between mentors and teams. Table-based seating with
track-labeled table tents is low-cost and directly actionable.

Theme 3: Student Preparedness for Sessions (1 mention)

-   *\"I would have liked to see students come to mentors with specific
    questions. It was a fun experience to get to treat the projects with
    a critical seriousness that got THEM asking more specific questions,
    but starting with those specific inquiries at the start is more
    helpful.\"*

This suggests adding a pre-visit prompt: before visiting a station,
teams should write down 1-2 specific questions. This simple mechanism
would increase session productivity and is consistent with the PI
framework's emphasis on \"Inquiring Deeply.\"

6\. Structural Assessment

6.1 What Worked

1.  **Station-expertise matching (4.80/5):** The stratified assignment
    process using mentor-declared interest areas produced high
    alignment. Keep this process.

2.  **Session length (4.67/5):** 15-20 minutes per team hit the sweet
    spot---enough for substantive guidance without consuming the
    mentor's full availability.

3.  **100% return intent:** Despite identified friction points, every
    respondent found the experience worthwhile enough to repeat. This is
    the strongest indicator of program health.

4.  **Project quality met expectations:** The curated problem bank, PI
    workshop, and team formation process collectively produced work that
    mentors found competent.

6.2 What Needs Attention

5.  **Feedback response rate (24%):** With 76% of attending mentors not
    providing feedback, the dataset is too thin to make confident
    claims. Future iterations need an on-site feedback
    mechanism---collecting responses before mentors leave rather than
    via post-event email.

6.  **Team-mentor utilization imbalance:** 50% of respondents saw 0-3
    teams. This represents wasted expert capacity. Contributing factors
    likely include poor wayfinding, student reluctance to seek help, and
    no structured rotation pushing teams to mentors.

7.  **Day 1 roaming coverage:** Only 1 roaming mentor for 25 teams on
    Day 1 is insufficient. Roamers serve a critical function for teams
    that don't self-select into stations---precisely the teams most
    likely to need help.

8.  **Pre-event context gap:** Mentors lacked knowledge of which teams
    were working on which problems. This is solvable with a simple
    team-problem assignment sheet distributed at check-in.

9.  **Physical layout friction:** Booth seating and absent station
    signage created unnecessary barriers. Tables with track-labeled
    tents are a simple fix.

7\. Recommendations for Next Iteration

Immediate / Low-Cost

10. **Pre-shift mentor briefing:** 15-minute huddle at station start.
    Distribute a one-page sheet listing each team's selected problem,
    table number, and a 1-line description. Define mentor scope (problem
    scoping, PI integration, technical guidance, pitch feedback).

11. **Table tents with station labels:** Replace booths with open
    tables. Large, visible station labels (\"Education & Operations,\"
    \"Student Wellness,\" \"Health & AI/Tech\") so teams can find the
    right mentor cluster from across the room.

12. **Pre-visit question prompt for teams:** Before visiting a mentor
    station, teams write 1-2 specific questions on a sticky note or
    card. This primes productive sessions and reduces vague "what do you
    think?" interactions.

13. **Venue wayfinding signage:** Directional signs from Skysong
    entrances to the event space, clearly marked mentor area, and a
    posted floor plan near registration.

Structural / Medium-Effort

14. **Mandatory mentor visit requirement:** Require each team to visit
    at least 1 mentor station on Day 1 and 1 on Day 2 as a checkpoint.
    This increases utilization and ensures struggling teams get support.

15. **Increase Day 1 roaming coverage:** Target 2-3 roaming mentors on
    Day 1 (matching Day 2 levels). Day 1 is when teams are scoping---the
    exact activity mentors identified as the primary need. Roamers are
    the proactive intervention mechanism.

16. **On-site feedback collection:** Collect mentor feedback via a
    QR-code survey at end of each shift rather than post-event. Target
    80%+ response rate. Consider a 2-minute \"exit survey\" as mentors
    check out.

Exploratory / Higher-Effort

17. **Mentor-team matching system:** Once teams select problems,
    auto-recommend specific mentors based on problem-expertise
    alignment. A simple Slack bot or posted matrix would suffice.

18. **Session tracking dashboard:** Log mentor-team interactions to
    identify which teams haven't visited a mentor and dispatch roamers
    accordingly. This converts reactive coverage into data-driven
    outreach.

8\. Limitations and Caveats

-   Sample size (n=6) precludes statistical inference. All findings are
    directional.

-   Self-selection bias: mentors who responded may skew toward those
    with stronger opinions (positive or negative).

-   The survey did not capture station assignment, day(s) worked, or
    mentor name---preventing cross-referencing with the roster for
    deeper analysis.

-   \"Teams met\" is self-reported and likely approximate. No
    interaction log exists.

-   The feedback form had no question about Day 3 (showcase/judging),
    limiting analysis to office hours only.

9\. Summary of Logic

The data supports three conclusions with high confidence despite the
small sample:

19. **The mentor program delivered value.** 100% return intent, 100%
    projects met expectations, and high expertise-match scores confirm
    the core model works.

20. **The primary failure mode is underutilization, not
    dissatisfaction.** Mentors were willing and capable; the bottleneck
    was teams finding and engaging them. This is a routing and discovery
    problem, solvable with better wayfinding, structured visit
    requirements, and pre-session question prompts.

21. **The feedback instrument itself needs redesign.** A 24% response
    rate means 76% of mentor experience data was lost. On-site
    collection, shorter surveys, and linking responses to station/day
    would dramatically increase both volume and analytical power.

*--- End of Report ---*
