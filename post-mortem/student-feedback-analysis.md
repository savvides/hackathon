**STUDENT FEEDBACK ANALYSIS**

ScaleU + Principled Innovation Academy Hackathon 2026

February 2--4, 2026 \| 1951@Skysong, Scottsdale, AZ

  -----------------------------------------------------------------------

  -----------------------------------------------------------------------

Post-Event Analysis \| Version 1.0

Prepared: February 5, 2026

**n = 16 respondents \| Response rate: 57% of 28 participants \| 9
teams**

**1. Executive Summary**

Sixteen of twenty-eight participants (57%) submitted post-event
feedback---a strong response rate that provides reasonable confidence in
the patterns below. The event ran with 28 students across 9 teams, well
below the design capacity of 100 students and 25 teams. This scale
difference is itself a significant finding: the gap between planned and
actual participation warrants separate investigation (registration
funnel, show rate, or scoped-down launch).

+-----------------+-----------------+-----------------+-----------------+
| **+38**         | **8.25**        | **4.39**        | **57%**         |
|                 |                 |                 |                 |
| Net Promoter    | Mean Recommend  | Mean Across All | Response Rate   |
| Score           | (0--10)         | Likerts         | (16/28)         |
+-----------------+-----------------+-----------------+-----------------+

The headline signal is positive: 50% of respondents scored 9 or 10 on
the recommendation question (promoters), and only two scored 6 or below
(detractors). The structural design choices---problem bank, mentor
hours, timeline---received consistently high marks. The two areas with
meaningful variance are team dynamics and PI framework engagement, both
of which correlate with overall satisfaction and warrant operational
attention for future iterations.

**2. Methodology and Limitations**

  -----------------------------------------------------------------------
  METHODOLOGY NOTE: 16 of 28 participants responded (57%), providing a
  majority sample. However, with only 28 participants across 9 teams, the
  absolute numbers are small---individual responses carry outsized weight
  (each response = 6% of the dataset). Cross-tabulations should be read
  as directional patterns, not statistically significant differences. The
  larger question---why the event ran at 28% of its 100-student design
  capacity---is outside this survey's scope but should be addressed
  separately.

  -----------------------------------------------------------------------

The survey contained six Likert-scale items (1--5, except NPS at 0--10),
three categorical-choice questions, and one open-text field. No
demographic data was collected alongside feedback, so cross-referencing
with team assignments, problem selection, or skill profiles is not
possible. With 9 teams total and 16 respondents, approximately 1--2
members per team are represented---enough to surface patterns but not
enough to reconstruct any single team's full experience.

**3. Net Promoter Score Analysis**

**3.1 Distribution**

+----------------------------+----------------------------------+------+
| 10 (Promoter)              |   -----------------------------  | **6  |
|                            |                                  | (38  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| 9 (Promoter)               |   -----------------------------  | **2  |
|                            |                                  | (12  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| 8 (Passive)                |   -----------------------------  | **2  |
|                            |                                  | (12  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| 7 (Passive)                |   -----------------------------  | **4  |
|                            |                                  | (25  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| 6 (Detractor)              |   -----------------------------  | **1  |
|                            |                                  | (6   |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| 4 (Detractor)              |   -----------------------------  | **1  |
|                            |                                  | (6   |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+

**3.2 Interpretation**

NPS of +38 places the event in a healthy range for a first-run hackathon
with pre-assigned teams and a mandatory ethical framework---two
structural friction points that typically depress scores. The bimodal
distribution (cluster at 10, cluster at 7) suggests two distinct
experience pools rather than uniform satisfaction.

The two detractors (IDs 14 and 15) both reported
***\"Struggled---significant conflict or disengagement\"*** on
collaboration quality. ID 15 gave the lowest NPS (4) and the only
1-rating on timeline sufficiency, indicating that team dysfunction
consumed time that should have gone to building. Both requested better
skills-based matching. This is the clearest signal in the data: **team
dysfunction is the primary driver of negative experience, and it
cascades into every other metric.**

**4. Structural Element Ratings**

**4.1 Summary Table**

  ------------------------ ---------- ------------ --------- --------- ----------
  **Element**              **Mean**   **Median**   **Min**   **Max**   **% Top
                                                                       Score**

  **Problem Bank (1--5)**  4.56       5            3         5         75%

  **Mentor Hours (1--5)**  4.44       5            1         5         75%

  **Timeline (1--5)**      4.38       5            1         5         62%

  **PI Workshop (1--5)**   4.31       5            3         5         62%

  **Pre-assigned Teams     4.25       5            3         5         50%
  (1--5)**                                                             
  ------------------------ ---------- ------------ --------- --------- ----------

**4.2 Key Observations**

**Problem Bank** was the highest-rated element (4.56). Three respondents
gave it a 3, and one open-ended comment noted the need for \"more
assignments that non-technical people can work with\" (ID 10). The
50-problem bank was designed with breadth, but the skill-level
appropriateness signal suggests some teams found their selected problem
either too technical or too domain-specific for their composition.
Cross-referencing problem selection with team skill profiles would
clarify this.

**Mentor Office Hours** scored 4.44 with 75% at the top mark. One
outlier rated 1 (ID 4), whose other scores suggest a generally
disengaged experience. ID 15 (the lowest NPS respondent) rated mentors 3
and specifically requested \"better mentoring regarding the development
process\"---suggesting the station-based model may not have served teams
in distress.

**Timeline Sufficiency** at 4.38 is strong, but the single score of 1
(ID 15, same detractor) combined with their team dysfunction suggests
that 2.5 days is sufficient when teams function, but becomes
catastrophically insufficient when they don't. This isn't a timeline
problem; it's a compounding effect of team failure.

**5. Team Dynamics: The Primary Lever**

**5.1 Collaboration Quality Distribution**

+----------------------------+----------------------------------+------+
| Highly effective           |   -----------------------------  | **9  |
|                            |                                  | (56  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Functional with friction   |   -----------------------------  | **3  |
|                            |                                  | (19  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Struggled                  |   -----------------------------  | **3  |
|                            |                                  | (19  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Mixed contribution         |   -----------------------------  | **1  |
|                            |                                  | (6   |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+

**5.2 Collaboration Quality → NPS Correlation**

  ---------------------------------- ------------------ ------------------
  **Collaboration Quality**          **Avg NPS**        **n**

  **Highly effective**               9.1                9

  **Functional with friction**       8.0                3

  **Struggled**                      6.0                3
  ---------------------------------- ------------------ ------------------

The gradient is stark: 9.1 average NPS for highly effective teams drops
to 6.0 for struggling teams. This 3.1-point NPS gap across collaboration
quality is the single largest effect size in the dataset. It exceeds the
impact of any individual program element (PI, mentors, problem bank) on
satisfaction.

**5.3 Team Formation Preferences**

+----------------------------+----------------------------------+------+
| Keep pre-assigned (no      |   -----------------------------  | **6  |
| change)                    |                                  | (38  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Better skills/interest     |   -----------------------------  | **6  |
| matching                   |                                  | (38  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Allow self-selection       |   -----------------------------  | **3  |
|                            |                                  | (19  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Smaller teams (2--3)       |   -----------------------------  | **1  |
|                            |                                  | (6   |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+

  -----------------------------------------------------------------------
  SIGNAL: All 3 students who \"Struggled\" requested better skills-based
  matching. Zero struggling teams said pre-assigned worked well.
  Conversely, 4 of 9 highly effective teams said no change needed. The
  pre-assignment model isn't broken---the matching algorithm is
  undertrained.

  -----------------------------------------------------------------------

The design document specifies hard constraints (each team has 1
technical member, mixed experience levels) but the registration form may
not capture enough signal for meaningful matching. The open-ended
responses reinforce this: ID 12 explicitly requested \"asking more about
a person's skills so it becomes fun for everyone.\" This isn't a
complaint about pre-assignment as a concept---it's a demand for better
inputs to the assignment algorithm.

**6. Principled Innovation Framework**

**6.1 PI Effect on Projects**

+----------------------------+----------------------------------+------+
| Significantly shaped       |   -----------------------------  | **8  |
| approach                   |                                  | (50  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Moderately influenced      |   -----------------------------  | **5  |
| thinking                   |                                  | (31  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+
| Felt like a checkbox       |   -----------------------------  | **3  |
|                            |                                  | (19  |
|                            |   -----------------------------  | %)** |
+----------------------------+----------------------------------+------+

**6.2 Workshop Rating → PI Effect Correlation**

  ---------------------------------- ------------------ ------------------
  **PI Effect**                      **Avg Workshop     **n**
                                     Rating**           

  **Significantly shaped**           5.0                8

  **Moderately influenced**          3.6                5

  **Checkbox requirement**           3.7                3
  ---------------------------------- ------------------ ------------------

Perfect bifurcation: every student who rated the workshop 5/5 reported
that PI significantly shaped their project. Students who rated the
workshop 3--4 split between moderate influence and checkbox. This
suggests the workshop is the causal mechanism---when it lands, PI
integration follows. When it doesn't land, PI becomes performative.

The 19% checkbox rate (3 students) is the operational concern. These
students likely received the same workshop content but either (a) didn't
internalize the framework's relevance to their selected problem, (b) had
team dynamics that deprioritized ethical reasoning in favor of building
speed, or (c) had prior exposure that made the workshop feel redundant.
Without demographic data, the cause is indeterminate.

**7. Open-Ended Response Analysis**

Eight of sixteen respondents provided open-text feedback. Thematic
clustering:

**7.1 Theme Clusters**

  ------------------ ------------------------------------ ---------------
  **Theme**          **Representative Quotes**            **Frequency**

  Team matching      \"Asking more about a person's       3 mentions
  depth              skills so it becomes fun for         
                     everyone\"                           

  Presentation       \"More questions after               2 mentions
  format             presentations\"; \"time gap between  
                     deadline and presentation\"          

  Non-technical      \"More assignments that              1 mention
  inclusion          non-technical people can work with\" 

  Scale / volume     \"Too many submissions\"; \"more     2 mentions
                     participants\"                       

  Prizes / logistics \"Better prizes\"; \"energy drinks\" 2 mentions

  Development        \"Better mentoring regarding the     1 mention
  mentoring          development process\"                
  ------------------ ------------------------------------ ---------------

**7.2 Actionable Signals**

**Presentation format** received two distinct requests: (1) longer Q&A
after showcase presentations (ID 11), and (2) a buffer between
submission deadline and final presentation (ID 13). The current design
has submission at 5:00 PM Day 2 and showcase at 10:00 AM Day 3---a
17-hour gap that includes overnight. ID 13's request may indicate teams
felt rushed setting up showcases on Day 3 morning rather than wanting
more development time.

**The non-technical inclusion comment** (ID 10) deserves attention given
the hackathon's emphasis on interdisciplinary teams. If problem scoping
and prototype building skew heavily toward code, non-technical members
(designers, domain experts, researchers) may feel marginalized. The PI
framework should theoretically create space for these contributions, but
it may not be functioning that way in practice.

**8. Detractor Profile Analysis**

The two detractors share a common failure mode but differ in specifics:

  ------------------- -------------------------- --------------------------
  **Dimension**       **ID 14 (NPS: 6)**         **ID 15 (NPS: 4)**

  **Pre-assigned      4/5                        5/5
  Teams**                                        

  **PI Workshop**     5/5                        4/5

  **Mentor Hours**    5/5                        3/5

  **Timeline**        3/5                        1/5

  **Problem Bank**    3/5                        5/5

  **Collaboration**   Struggled                  Struggled

  **Team Change**     Better matching            Better matching

  **PI Effect**       Significantly shaped       Moderately influenced
  ------------------- -------------------------- --------------------------

ID 14 rated the PI workshop and mentors at 5/5 but still struggled with
collaboration and found the timeline insufficient. This profile suggests
the student valued the program's intellectual design but was defeated by
team dynamics---a motivated participant trapped in a dysfunctional team.

ID 15 is the more alarming case: timeline rated 1, mentors rated 3, and
explicitly requested \"better mentoring regarding the development
process\" and \"a demo pitch before the actual one.\" This student
needed intervention-level support that the station-based mentor model
didn't provide. The roaming mentor role was designed for exactly this
scenario, but either didn't reach this team or wasn't sufficient.

**9. Recommendations**

  -----------------------------------------------------------------------
  These recommendations are ordered by expected impact on future NPS, not
  by implementation difficulty. The team dynamics cluster accounts for
  the majority of variance in satisfaction.

  -----------------------------------------------------------------------

**9.1 High Priority: Team Formation Algorithm**

1.  **Expand registration intake:** Add self-assessed technical skill
    domains (frontend, backend, data, design, domain research), working
    style preferences (structured vs. fluid), and a brief free-text on
    what they hope to contribute. The current \"technical vs.
    non-technical\" binary is too coarse.

2.  **Run team compatibility scoring:** Rather than purely random
    stratified assignment, weight matching to maximize skill
    complementarity within teams. Even a simple algorithm (ensure each
    team has frontend + backend + domain + wildcard) outperforms random
    stratification on a single dimension.

3.  **Evaluate optimal team size for actual turnout:** The design
    document specified teams of 4 for 100 students (25 teams). The
    actual event had 28 students across 9 teams, meaning some teams
    likely had 3--4 members. If future events run at similar scale,
    fewer larger teams (e.g., 5--6 teams of 5) may be
    preferable---redundancy absorbs dysfunction. The one request for
    \"smaller teams\" is outweighed by the dysfunction signal.

**9.2 High Priority: Distressed Team Detection**

4.  **Formalize the Day 2 status check-in as a triage tool:** The
    current 1--5 team dynamics rating at 8:30 AM is the right mechanism.
    Add an automatic threshold: any team reporting 2 or below gets a
    mandatory organizer conversation within 30 minutes. Don't wait for
    teams to self-identify---they won't.

5.  **Equip roaming mentors with a distress protocol:** Roaming mentors
    should proactively check teams that haven't visited any station by
    mid-Day 1. Lack of help-seeking is a signal, not evidence of
    self-sufficiency.

**9.3 Medium Priority: PI Workshop Differentiation**

6.  **Add a mid-hackathon PI checkpoint:** The 19% checkbox rate may
    reflect students who engaged with PI during the workshop but lost it
    during build pressure. A brief structured reflection at the Day 2
    morning check-in (\"Which PI principles have you applied? Which
    haven't you touched?\") could re-anchor the framework without adding
    burden.

7.  **Provide problem-specific PI prompts:** Each problem in the bank
    already includes ethical considerations. Convert these into
    structured worksheets that teams fill during development, not just
    during the workshop. This shifts PI from a front-loaded lecture to a
    distributed practice.

**9.4 Medium Priority: Presentation Format**

8.  **Extend Q&A time in showcase:** The current 5--6 minutes per team
    during table showcase is tight. If judge count permits, consider
    8--10 minutes to allow deeper questioning. This also gives teams
    better signal on their own work.

9.  **Add optional Day 2 afternoon practice pitches:** ID 15 explicitly
    requested \"a demo pitch before the actual one.\" A voluntary
    2-minute practice pitch to a mentor between 3:00--4:00 PM Day 2
    costs nothing and gives struggling teams a reality check before
    submission.

**9.5 Low Priority: Survey Design**

10. **Maintain in-event survey administration:** The 57% response rate
    is strong. Continue collecting feedback close to event conclusion.
    Consider administering during the closing session (before prize
    announcement) to push toward 80%+.

11. **Add team ID and problem number to the survey:** This single change
    would enable cross-referencing feedback with team composition,
    problem difficulty, and outcomes---transforming anecdotal signal
    into actionable diagnostics.

**10. Summary of Logic**

-   The hackathon's structural design (problem bank, mentors, timeline,
    PI workshop) is validated---all elements scored above 4.25/5.

-   Team collaboration quality is the dominant variable explaining
    satisfaction variance: 3.1-point NPS gap between effective and
    struggling teams.

-   The pre-assignment model itself is not the problem (38% said no
    change needed). The matching algorithm's inputs are the
    problem---not enough signal to form complementary teams.

-   PI framework engagement is bimodal and tracks workshop reception.
    The 50% \"significantly shaped\" rate is a strong result for a
    mandatory framework; the 19% checkbox rate is the improvement
    target.

-   The two detractors are identifiable failure modes (team
    dysfunction + insufficient intervention), not evidence of systemic
    design failure.

-   The 57% response rate is solid, but the small absolute sample (n=16)
    means individual responses carry 6% weight each. The larger open
    question is why the event ran at 28% of design capacity (28 of 100
    planned participants).

**Appendix A: Raw Score Distributions**

**A.1 NPS Distribution**

  ----------- ----------- ----------- ----------- ----------- -----------
  **Score 4** **Score 6** **Score 7** **Score 8** **Score 9** **Score
                                                              10**

  1 (6%)      1 (6%)      4 (25%)     2 (12%)     2 (12%)     6 (38%)
  ----------- ----------- ----------- ----------- ----------- -----------

**A.2 Likert Distributions (1--5 Scale)**

  ------------------------- --------- --------- --------- --------- ---------
  **Element**               **1**     **2**     **3**     **4**     **5**

  **Pre-assigned Teams**    0         0         4 (25%)   4 (25%)   8 (50%)

  **PI Workshop**           0         0         5 (31%)   1 (6%)    10 (62%)

  **Mentor Hours**          1 (6%)    0         2 (12%)   1 (6%)    12 (75%)

  **Timeline**              1 (6%)    0         1 (6%)    4 (25%)   10 (62%)

  **Problem Bank**          0         0         3 (19%)   1 (6%)    12 (75%)
  ------------------------- --------- --------- --------- --------- ---------

**A.3 Complete Open-Ended Responses**

  -------- --------- -------------------------------------------------------
  **ID**   **NPS**   **Suggested Change**

  2        9         Better prizes.

  7        10        I think there may have been too many submissions

  8        10        All was great, maybe have energy drinks next!

  10       10        Ensuring even distribution of backgrounds; more
                     assignments non-technical people can work with.

  11       10        More questions allowed after presentations for product
                     clarity.

  12       8         Change team assignment by asking more about skills so
                     it becomes fun for everyone.

  13       8         Time gap between final deadline and final presentation.

  15       4         More participants, better development mentoring, demo
                     pitch before the actual one.
  -------- --------- -------------------------------------------------------
